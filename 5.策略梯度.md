# 5. 策略梯度（Policy Gradients）

本文主要包含几个方面：

1. 策略梯度算法介绍。
2. 策略梯度如何工作。
3. 如何降低方差。

## 1. 策略梯度介绍

### 1.1 强化学习目标函数评估

其中，我们使用NN去拟合策略$\pi$, 网络的参数表示为$\theta$,输入为状态s，输出为动作a。动作a作用于环境，环境根据状态转移分布$p(s'|s,a)$而产生下一个状态，并再次通过NN进行拟合。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-72bfb675dd2a5f98.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如前文所述，我们把RL的目标函数定义为如下表示，其目标为最大化期望回报，找到让期望回报最大化的最优参数$\theta$.

![image.png](https://upload-images.jianshu.io/upload_images/15463866-af547edfe6d0e8bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们把目标函数区分为有限时间步和无限时间步两种情况。

对于无限时间步：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-e8908b3b90f6075f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于有限时间步：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-11c8fe9c854f845f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 1.2 评估策略梯度

**代价函数定义**：

根据我们的目标，我们把 代价函数$J(\theta)$定义为如下公式，其中我们使用采样的方法，将所有来自$\pi_{\theta}$的reward都全部叠加起来。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-baeae130f1a8c917.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**直接对策略进行微分:**

依据定义的代价函数，优化目标更新为了：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-ae2bfa2898c404ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

然后我们对$J(\theta)$直接进行微分：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-8a16e8653bd89d5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



这里我们使用了一个对log函数微分的小技巧：
$$
\pi(\tau){\bigtriangledown}_{\theta}log\pi_{\theta}(\tau)=\pi_{\theta}(\tau)\frac{{\bigtriangledown}_{\theta}\pi_{\theta}}{\pi_{\theta}(\tau)}={\bigtriangledown}_{\theta}\pi_{\theta}
$$
我们继续对上述的梯度进行展开，其中用到：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-543d48f7fb403ba5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中可以消除掉与参数$\theta$无关的部分，最后代价函数的梯度可以做到简化。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-6a83b1f5f9e14892.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**REINFORCE算法**

到这里我们总结一下之前的描述。

代价函数：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-29aba7475486ed81.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

代价函数的梯度：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-7fbba2059721724e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

使用采样的方法得到期望：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-e2802c65373a399c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

使用梯度下降算法更新梯度：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-d5b0b0d49bb07b8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

最后我们可以得到最原始的REINFORCE算法：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-b999433adf254b36.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



### 1.3 理解策略梯度

官代价函数梯度中的log部分如何理解呢?

![image.png](https://upload-images.jianshu.io/upload_images/15463866-46dfa41a5c9f7a35.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这个算法相当于对于表现的比较好的trajectory，增大其出现的可能性，而表现得比较差的就减小其可能性。与最大似然的方法相比区别在于对每个样本是区别对待的。这个概念也就是trial and error的想法。  

 直接进行最大似然的方法相比，策略梯度法的梯度仅仅是相当于对每一项使用了对应序列reward和来进行加权。

他们的区别如下：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-6ee259ecafd973c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 1.4 关于部分观测

对于部分观测observation, 他们可能不适用于马尔科夫性质，但是我们仍然可以使用策略梯度算法作用于POMDP的情况。

### 1.5 策略梯度有哪些问题

主要问题就是高方差。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-aa1199e006d5d088.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

假设序列分布作为横轴，Reward作为纵轴。如果有三个采样点如绿色横线表示，有两个表现为正，一个为负，那么下一次的梯度更新后的分布就会往右边两个靠近，远离左边。

但是如果对每个序列的Reward同时加一个常数，就会变成黄色的表示，那么这三个点更新后的概率分布都会变大。对于极端的例子比如把右边两个例子的反馈减为0，那么更新后的分布就不会关注这两个例子。显然这是不对的，因为实际上的策略不应该被反馈的绝对值影响，而只关注它们的相对值。

## 2. 降低方差

### 2.1 策略梯度中的高方差问题

 直接使用策略梯度算法REINFORCE会导致更新梯度的时候方差太大 。

### 2.2 Causality分析

**Causality**：这是指在时间t+1时发生的事情不会影响之前发生的事情。因此在代价函数中我们只考虑“reward to go”部分。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-bd5490c6f6367aa7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 2.3 Baselines

在实际应用中，我们通常会把reward减去一个baseline以改进REINFORCE算法。这个baseline一般可以选择为平均reward。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-9609d8db6efabec8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中的baseline可以表达为：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-131c2ce1d87c30b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

加了baseline并不会影响我们对梯度的计算，其证明过程如下：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-3c928252651504aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

所以我们之前减去的baseline在期望上其实是unbiased的。虽然把平均reward作为baseline并不是最佳的方式，但是在实际中应用起来很不错，又很简单。

### 2.4 baseline的方差分析

![image.png](https://upload-images.jianshu.io/upload_images/15463866-99c7561511ec92cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

可以看到最后的结果相当于是序列对应的概率的log求梯度之后的平方加权的reward的加权期望。计算起来稍微复杂了一些，带来的增益没那么大，因此我们一般还是会选择直接使用期望和的平均值。

## 3. On-policy与off-policy

之前的问题设定中,Policy Gradient是一个On-policy算法，其中我们不能忽略的是$\tau \sim\pi_{\theta}(\tau)$。应用中我们会使用神经网络来拟合$\theta$，但是神经网络在每个gradient step中只能改变一点点。On-policy的学习方法是极度没有效率的。

### 3.1 重要采样（Importance Sampling）

![image.png](https://upload-images.jianshu.io/upload_images/15463866-e5fbda7e5ba1dbf7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在实际应用中为了使用off-policy,我们会使用到IS的方法，将代价函数转换为：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-96aed468304aad99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这里会有一点问题：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-5fbdaf6becc9b61d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

IS中涉及到累乘很多项，如果其中涉及到概率为0的情况，那就没有效果了。

### 3.2 off-policy下的策略梯度

![image.png](https://upload-images.jianshu.io/upload_images/15463866-0d200ad3d8bf700b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们的策略梯度可以简化为：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-61f35471230d5fc8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 3.3 on-policy与off-policy的比较

![image.png](https://upload-images.jianshu.io/upload_images/15463866-1e1dd5cfa1f070f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

---

## 4. Policy Gradient在实际中的应用建议

* 一定要记住梯度的方差很高，这点和监督学习不同，梯度的造影影响非常大。
* 建议使用大批量进行计算。
* 微调learning rate是非困难的，所以尽量使用自适应学习率的优化器，比如Adam.