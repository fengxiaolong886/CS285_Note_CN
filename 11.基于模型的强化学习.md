# 11. 基于模型的强化学习（Model-Based Reinforcement Learning）
本节主要介绍基于模型的强化学习，主要包括

* 基本的基于模型的强化学习算法： 如何学习模型，使用模型进行控制
* 讨论基于模型强化学习算法的不确定性（uncertainity）
* 讨论基于模型强化学习算法在复杂观测中的情况

## 一、原始模型(Naive model)
如果我们知道$f(s_t,a_t)=s_{t+1}$（或者$p(s_{t+1}|s_t,a_t)$为随机的情况），原始算法最直观的思路是：首先运行策略，通过与环境交互获得数据，利用它们去拟合模型，然后通过之前《最优控制与规划》中介绍的方法选择决策动作。其基本流程如下版本０.５的算法．
![image.png](https://upload-images.jianshu.io/upload_images/15463866-057e8dfc93acd634.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这也是在传统机器人领域做系统识别（system identification）的方法，如果能够有精心设计的动态表征（dynamics representation）以及好的基础策略，将非常有利于提高学习速度。

但是这种简单方法的缺点与模仿学习（Imitiation Learning）一样有分布不一致(distribution mismatch)的问题。比如如下这张图，黄色是真实的奖励函数，其曲线先上后掉落。但是基础策略的探索仅仅局限于前面上升的部分，在这个部分学习到的分布与实际的分布的差距很大。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-74db4553e8f5cc37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

联系到在模仿学习中的DAgger算法，其通过人工对新的数据进行再标记，从而修正这种问题。在这里同样可以采用这样的思路，DAgger是数据的标签不准确，而此处是模型不准确。如果我们可以根据实时返回的新数据进行模型的更新，就可以保证数据的正确性，这样就产生如下版本１.5的算法。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-6c121b38bb165bdf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这里还是存在一个问题，正常情况下是要对模型进行一个完整的规划过程，然后将所有的决策动作执行完才进行下次的拟合，也就是个开环（open-loop）规划的形式。由于模型存在误差，则会导致规划的误差累计，从而使得规划后面的数据质量变差。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-8f740b7edeed51ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

##  二、重新规划（Replan）
基于上面的问题，引入replan，每次规划后的结果只选择第一个动作执行，然后重新进行规划：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-bcab239e08094981.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这里的重点就在第三步，在第四步执行了首个动作后，将数据添加到缓存中，就可以进行下一次的规划。Replan进行得越多，对于模型以及单次规划的质量要求就更低。在很多时候，即使只是随机采样（random sampling）都可以达到不错的效果。

##  三、强化学习中的不确定性（ Uncertainty in model-based RL）
### 直观解释
**先从直觉上来看看不确定性**,前面引入了replan版的MBRL算法，看起来似乎是很完美，而在实际试验中会发现它往往效果都比较差，如下图：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-ed45df23a2004148.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这篇Nagabandi, Kahn, Fearing, L. ICRA 2018的图是在Cheetah环境上跑的结果，前面绿色是纯model-based的结果，它已经陷入了局部最优，而经过distillation处理之后训练一个model-free的模型，会发现它其实比纯model-free效果要好，说明它其实是有学习的潜力在里面的。但是为什么会停止在这个点呢？一个解释是，在前面的规划中由于model存在误差，而由于规划的时候都是在最大化奖励值，这往往会使得模型倾向于乐观估计，这个也类似于DQN的over estimation的原理，从而许多时候会作出相对错误的决策。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-bdf1e1fe47921ecd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如何求解这个问题？后面的主要方法整体而言是基于这样一个观察：对于同一个点（奖励值的理论均值一样），方差比较高的预测的样本累积奖励都比较高。例如下图中，假设目标点处在悬崖边上，对于两个预测模型来说，理论上做动作的平均奖励都是一样的，但是对于方差高的预测模型，那么它会更有可能掉入悬崖中，获得比较低的奖励，从而样本累积奖励会比较低。因此在同一个点，最大奖励是可能一样的，但是平均大则代表方差比较小。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-837d79f1b18fadc7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

所以将uncertainty纳入考虑中，利用平均奖励替代最大奖励更有利于降低方差，既不过于乐观，也不过于悲观。

### 主要思想
在介绍具体的idea之前，首先介绍两种形式的uncertainty，包括statistical uncertainty和model uncertainty。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-ec88cf080c879a26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

前者通常是描述由于数据的统计指标上的问题导致模型的uncertainty，例如如果数据本身噪音比较大，对于一个自变量它的因变量方差很大，那么学习到的模型对点的预测就会存在很大的uncertainty。

而model uncertainty则是形容模型对自己预测的确信程度，它通常从模型本身的性质得到的，例如Gaussion process就是通过在接触过某个区域的点的数量来得到这个区域的uncertainty。在诸如神经网络之类的模型中通常是没有这一项的。

例如在上面的悬崖的例子中，如果使用神经网络来输出，那么它的statistical uncertainty可能会比较小，也就是说它更倾向于乐观估计，选择迈向终点。但是如果在探索次数比较少的情况下，它的model uncertainty其实会是非常大的。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-d6e116a264860159.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**神经网络输出层使用entropy**
第一个想法，就是通过判断output的entropy，也就是描述output的不确定程度，这个其实就是在描述statistical uncertainty，通过上面的描述也知道，仅仅刻画这种uncertainty是不够的，所以需要找到方法刻画model uncertainty，并将两者结合：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-eb692cc19bf48cfe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**评估模型的不确定性**
一般情况下，我们做估计其实是为了达到这么一个目的：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-3777c18d60bf3c23.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
也就是说参数和数据可以相互生成，给定一批数据，一定可以导出某个参数，给出这个参数，则一定可以生成对应的一批数据，也就是数据与它的分布一一对应。从强化学习的角度来看，可以理解做closed-loop的过程中，通过策略与环境交互得到数据，利用这些数据修正策略，这分别就是右左两遍的式子，最终网络收敛，也就是数据与参数都趋于稳定，最终等号也就成立了。

在这里，如果希望考量model uncertainty，那么可以将前面一部分摘出来，估计利用数据生成一批参数（也就是一个新的模型）的概率，这也就是model uncertainty。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-e72bbad17fb09033.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
然后将它与statistical uncertainty结合在一起，得到模型在不同的参数下的期望概率，这也就是将两种uncertainty结合的方法：。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-34f47782dd8c74df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 模型

接下来就介绍两种实现上面主要思想的模型。

**贝叶斯神经网络（Bayesian neural networks）**
首先就是BNN，在常规的NN中，节点之间相邻的边都是通过weight进行连接的，它是一个数值。而在BNN中则是通过distribution进行相连，每过一条边相当于过一个分布，利用分布的方差，就可以衡量variance：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-7a67bc81c608cbdb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
通常而言，其中做的approximation就是引入某种分布，并利用分布输出的乘积表征model uncertainty，如下图中分布的mean表示的就是expected weight，variance表征的就是关于weight的uncertainty。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-0babdd05b10bc838.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**Bootstrap ensembles**
第二种方法则是引入机器学习中常见的ensembles方法：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-c36920b95ee934f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
通过多个model进行加权，从而减少方差，因此model uncertainty就可以描述为多个predict的mean：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-f337c4cabb5a1c5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
而整体刻画也即是output的mean：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-23c35f725a6511f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
它需要注意的是不同的模型之间是要独立的，故而数据也是要独立的，在传统方法中通常用sampled with replacement的方式，也就是有放回重采样的方式保证independent。而在神经网络中，由于随机初始化，以及SGD本身的随机性，所以model就以及足够独立的，因此也可以忽略此步。

**使用uncertainty做规划**
在上面提到的两类模型中，都是经过N个部分的平均得到最终结果，故而整体而言的目标函数转化成了：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-f4ddd512d1596557.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
因此整体的流程也就可以描述成如下的形式：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-83aa5c1b92ddf51f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
首先从data生成parameter的分布中采样出一组分布，通过model得到transition，相乘得到probability，计算累积reward，经过多次迭代得到平均累积reward。


## 四、Latent space models
在前面通过uncertainty的角度来提高算法的表达能力，但是通常就直接假设dynamics的learning是能够做得比较好的，但是在复杂场景中的dynamics学习实际是比较困难的：environment可能是partial observability的，state可能是高维且存在较多冗余的。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-229acb9bd7292f00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
所以在这节中则是会考虑如何从结构设计的角度使得model的学习变得更加容易。

### Latent space models
回顾MDP的结构图：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-5904fa26afc3c785.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
在partial observability的环境中，agent从environment中获取到的observation后，如果需要建模model，首先要处理得到的信息，需要学习observation model，也就是如何从observation中得到与decision相关的state，这个部分是一个高维到低维的映射。接着基于这个observation model进一步学习dynamics model，得到关于environment如何产生transition的过程，并同时学习reward model，得到关于reward的信息。

在上面的描述中，其实可以看出它的核心步骤是如何做从observation到state的mapping，这个也就是latent space model的名称由来，latent space就是表示state space。

从model fitting的目标函数来看，最基本的方式就是对sample得到的transition做maximum likelihood estimation：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-c70323d0bb7fd301.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

而在latent space models中，将原始的dynamics拆分成observation model与dynamics model，所以这里需要MLE的是两者的乘积：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-e496008016f8c607.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
这里的expectation是针对这个分布的：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-b09a31decc991a52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
如何理解这个sample，它其实就相当于是首先从所有的action和observation中学习到一个关于transition的分布，然后利用这个进行sample。
### MBRL with latent space models
** Latent space MBRL with dynamic**
针对上面提到的目标函数来看，它的重点在于如何学习这个分布:
![image.png](https://upload-images.jianshu.io/upload_images/15463866-790d2b7d992a636f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
在常规模式中，通常是学习一个approximate posterior :
![image.png](https://upload-images.jianshu.io/upload_images/15463866-40a2b44c3348f439.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
作为一个encoder。

第一个点是学习到这个encoder是怎么用到它的？假设现在有t+1步之前的sample，那么就可以用t步前的sample得到state t，从t-1步前的sample得到state t+1，从而得到同一套的state transition，从而能够近似上面的分布。

第二个点是这个posterior的形式，是否需要condition on这么多输入？这个其实也是类似MC与TD的争议，使用越多的输入，信息越多，自然就越准确，但是学习起来就会比较复杂。在本节中处于简单考虑，我们都假设仅仅使用当前的observation作为输入，从而expectation的对象就变成这种形式：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-dbc3fc3cff8be4fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/15463866-53aeae8379f1ccfa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
讨论完它的形式以及如何使用它，接下来就讨论如何如何学习它。同样也是为了简化讨论，这里假设observation model是deterministic的，也就是observation与state是一一对应的，因此encoder就可以直接转化为function的形式，在lecture14中会具体讨论stochastic case的情况。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-a05fd0be4c1e5e54.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
故而目标函数也就转化为，其中的每个部分都是differentiable的，这也就可以直接用BP来训练。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-d4d8cfcc70fac870.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
其中第一部分是通过transition进行约束，第二部分则是通过reconstruction的约束保证映射过程可逆，通常其实可以提高学习的稳定性。

** Latent space MBRL with reward model **
在上节中，利用approximate posterior将MDP做了简化，得到了新的目标函数：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-beba2880d98076ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在这节中，则是考虑引入reward model相关的信息，其实也就是在目标中加了reward model相关的学习：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-80012c71ea0c151e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
将它嵌入前面的model-based RL的流程中，这节其实就是考虑到observation的复杂性，利用latent space的方法，将步骤二的fit model变得更容易学习了：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-3b816ea3aa79f8e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)