# 6. Actor-Critic

本文主要介绍如下几个内容：

* 如何使用Critic去提升Policy Gradient的能力
* 如何评估策略（the policy evaluation problem）
* Discount因子
* 介绍Actor-Critic算法

---

# 1. 使用Critic提升Policy Gradient

首先我们还是回顾一下之前提到的REINFORCE算法：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-b95b09f736efb153.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在这个算法的第二步骤里面我们引入了“reward to go”这一项，该$\hat Q^\pi_{i,t}$表示了从当前的时间步t开始，所有的reward的期望之和。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-7fdb17d683314057.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们可以把这个由casuality引出的期望称之为“true expected reward-to-go”， 之所以我们这里考虑的是期望，是因为我们在实际中每个trajectory采样出来都是不一样的，我们需要把这不同的采样结果进行最后的平均以求期望。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-adf6737777687f6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**优势函数（Advantage function）**

我们在policy gradient的方法中为了降低variance，也考虑过引入一个基线来减少梯度的方差。这里我们更进一步一点，我们使用$Q^\pi(s_t,a_t)$ 代替原来的"reward to go"，并且使用值函数（V function）去代替原来的baseline,这样我们就有了新的估计值，即优势函数。

![image-20191213223412091](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20191213223412091.png)

与原始版本的baseline相比，原来的估计是无偏估计，但是在单个采样估计中具有很高的方差（variance），现在使用了优势函数之后可以降低方差。他们的比较如下图：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-d660085d7cec3123.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**三个函数的比较：Q, V and A**

在模型拟合的阶段，我们需要去评估结果，这个时候就要考虑去拟合Q, V 还是A。他们之间是有密切关系的：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-81da0274e2975172.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中对于Q函数他是在已经确定了$(s_t,a_t)$的情况下，并且已经产生了$r(s_t,a_t)$,即我们可以写为:

![image.png](https://upload-images.jianshu.io/upload_images/15463866-4f8aebd8b5e365b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

既然他们是有关系的，那么我们在实际使用的时候只需要计算一个期望函数$V^\pi(s_t)$.

# 2. Policy Evaluation（策略评估）的问题

对于如何去评估一个Policy是好是坏，我们从$V$的定义可以知道，强化学习的目标函数其实就是这个值关于初始状态$s_1$ 的期望值。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-19a35c8ba1c5c466.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这里有两种Monte Carlo的方法来评估：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-9bb5477dd304150c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

 第一种是在一个sample里面累积 reward，第二种还考虑了多个sample的平均。

我们从上面的第一种简单得到$V^\pi$的方法中得到了训练数据，我们就可以把评估的问题转换成了一个监督学习的问题：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-c125c665e0e2f85c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

形式化理解为，我们用同一个函数去拟合了很多很多的样本。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-429daa522ca6d52e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上面使用的方法会有很高的方差，在实际中我们是用一种近似的方式去减少方差：

![image-20191213225956284](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20191213225956284.png)

这里我们是直接使用了之前的V值（$\hat V_\phi^\pi$）去近似，然后得到了训练数据的样本$y_i$,这种方式称之为bootstrap。

# 3. Actor-Critic Algorithm

有了前面的基础，介绍了如何去拟合$V$，计算优势函数我们就能够导出Actor-critic算法，

![image.png](https://upload-images.jianshu.io/upload_images/15463866-c4cd806e16fd4936.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 4. 折扣因子（Discount Factor）

上面的actor-critic算法里第一步还需要采样一整个trajectory。想要变成每次只采样一个状态就需要先引入Discount factors的概念。

因为值函数V的定义是当前状态以后所有反馈值的和，在有限步长的任务中没有问题，但是如果是一个无限步长的任务，那么这个值有可能是无限大的。因此需要引入一个折损系数$\gamma$，它的意义在于让离当前状态比较近的反馈值更重要，而离得比较远的可能不那么看重.

![image.png](https://upload-images.jianshu.io/upload_images/15463866-6cf26cae2bc7a9cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上面举了个例子，在机器人做操作这种episodic tasks的时候是有限步长的就不需要discount factor，但是另外一种continuous任务，就不需要设定episodic所以这种情况加入discount factor就很重要。

加入了Dicount factor，我们对应的目标函数也变化：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-89f1fd4ae5641250.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![image.png](https://upload-images.jianshu.io/upload_images/15463866-6897409c8778b0ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这种情况下MC policy gradients有两种选择：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-8f8291c017c4d642.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

第一种是直接从当前时间t开始加系数$\gamma$，而第二种是从最开始t=1就开始加系数$\gamma$ 。然后再通过利用causasity去掉$t'$之前的反馈值。这样最终两种写法的系数还是有一些差别。

一般情况下两种方式有两种不同的解释和应用场景。第二种写法是对应着带有死亡状态的MDP形式。系数从第一步就开始加入，这就意味着这种写法更在意从头开始的动作，对于往后的动作给的关注更少。

而第一种写法是从时刻t开始加系数，也就是说它会一直在意从当前时刻开始的动作。这种形式一般用在一直连续运动的场景里。

第一种写法实际上不是一个正确的加了discount factor后的写法。它相当于是对平均反馈值加了一个系数来减小方差，它去除掉那些距离太远的反馈值的影响，因为可能太远了已经没有了意义。当然这样会是平均反馈的有偏估计。

第一种写法实际中更常用，也就是作为减小方差的方式。而第二种写法能够向我们解释在经典的场景里discount factor的意义。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-c7f81559f8f3b07c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



#  5. Actor-critic algorithms (with discount)

加入了discount factors之后的actor-critic算法可以采用对每个状态进行采用的形式，这样就有了online形式的算法，两种算法对比如下

![image.png](https://upload-images.jianshu.io/upload_images/15463866-bfede15d01a073d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 网络架构设计

 在实际实现actor-critic算法的时候可以选择两种结构。一种是让策略函数与值函数分别训练。这样做可能比较简单而且稳定，但是这样就不能共享一些提取特征的网络层。第二种是两种函数共享一部分网络，这样就能够共享前面提取特征的部分。 

![image.png](https://upload-images.jianshu.io/upload_images/15463866-86d74a676ab93eb2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 同步与异步

实际中如果实现一个online形式的算法，最好的做法并不是对每一个状态都做一次更新，而是得到足够多的样本里作为一个batch来更新。因为这样能够减小更新的方差。而实现这样的方式也有两种，一种是同步的一种是异步的。  

![image.png](https://upload-images.jianshu.io/upload_images/15463866-daa3755a7456779e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 6.Bias and variance(偏差与方差)

我们把AC和PG对比一下，AC一般会因为有critic所以有更低的方差，但是如果critic不好那么他的bias会很大。PG是没有bias的，但是他的方差很高。把他们结合在一起就能产生比较好的unbias,低方差的算法。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-59d949da3ad2a9fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 7. Q函数作为baseline

 之前用到作为baseline的函数一直都是V，实际上Q也能够作为baseline。只不过这样做实际上得到的不是一个advantage函数，在期望上得到一个期望为0的函数。因为减小了这部分的值，就能够减小对应部分的方差。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-5c90b5d30da532f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

但是期望为0直接带入得不到目标函数的梯度值，因此计算梯度值的时候还需要把$Q_\phi^\pi(s_t,a_t)$以期望的形式修正回来，这样目标函数梯度值的期望与原来保持一致

# 8. 其他改善的方法

 目前我们有两种得到advantage函数的形式，一种$A_C^\pi$是bootstrap的，有更低的方差，但是有比较高的偏差。第二种是蒙特卡洛采样减去估计值的$A_{MC}^\pi$，这样做没有偏差，但是方差比较大。因此我们需要想办法把这两种结合起来。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-ab57b6e96700e126.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们可以使用n-step和Eligibility trace的方法来改善。

使用n-step的直觉如下图：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-25103af79ebdc754.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![image.png](https://upload-images.jianshu.io/upload_images/15463866-437a2bb4ec261666.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)