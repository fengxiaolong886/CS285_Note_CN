# 6. Actor-Critic

本文主要介绍如下几个内容：

* 如何使用Critic去提升Policy Gradient的能力
* 如何评估策略（the policy evaluation problem）
* Discount因子
* 介绍Actor-Critic算法

---

# 1. 使用Critic提升Policy Gradient

首先我们还是回顾一下之前提到的REINFORCE算法：

【IMAGE】

在这个算法的第二步骤里面我们引入了“reward to go”这一项，该$\hat Q^\pi_{i,t}$表示了从当前的时间步t开始，所有的reward的期望之和。

【image】

我们可以把这个由casuality引出的期望称之为“true expected reward-to-go”， 之所以我们这里考虑的是期望，是因为我们在实际中每个trajectory采样出来都是不一样的，我们需要把这不同的采样结果进行最后的平均以求期望。

【image】关于Q

**优势函数（Advantage function）**

我们在policy gradient的方法中为了降低variance，也考虑过引入一个基线来减少梯度的方差。这里我们更进一步一点，我们使用$Q^\pi(s_t,a_t)$ 代替原来的"reward to go"，并且使用值函数（V function）去代替原来的baseline,这样我们就有了新的估计值，即优势函数。

【优势函数】

与原始版本的baseline相比，原来的估计是无偏估计，但是在单个采样估计中具有很高的方差（variance），现在使用了优势函数之后可以降低方差。，他们的比较如下图：
【比较】

**三个函数的比较：Q, V and A**

在模型拟合的阶段，我们需要去评估结果，这个时候就要考虑去拟合Q, V 还是A。他们之间是有密切关系的：

【QVA】

其中对于Q函数他是在已经确定了$(s_t,a_t)$的情况下，并且已经产生了$r(s_t,a_t)$,即我们可以写为:

【Q】

