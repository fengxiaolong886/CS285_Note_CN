# 强化学习简介
---
## 简介

本节课程主要包括这几个部分：
1.  定义什么是Markov decision process
2.  定义什么是RL问题
3.  宏观的角度看RL算法逻辑
4.  简要介绍RL算法的分类
这节的主要内容是关于RL。RL问题是一个优化问题，但首先需要定义需要优化目标是什么，给RL做一个定义。然后从宏观上总结一下不同种类的强化学习算法，但是不深入算法本身。
---
# 1. MC,MDP与POMDP的定义

#### 1.1 基本的问题定义

在之前的模仿学习里面我们已经知道了Sequencial Decision的问题。也就是不断地根据当前时间的$ o_{t}$ 来作出决策动作$ a_{t}$，然后在给定$ o_{t}$ 之下$ a_{t}$的分布，即$\pi_(a_{t}|o_{t})$。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-f0ce8262dc7d712e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
强化学习算法就是通过学习Parameter来得到一个策略$\pi$。$\pi$可以是依赖于当前的完整状态$ s_{t}$,也可以是部分观测状态$ o_t$。

在上一篇中，我们也谈过Markov Property。也就是$s_3$是由$s_2$与$a_2$来决定，他和之前的$s_1$那些状态没有关系，是独立的。

不考虑之前的Imitation Learning，如果我们没有人类经验输入，怎么让机器学习到我们需要的策略$\pi$呢？

我们首先要定义一个reward函数，这个reward函数来评估策略$\pi$的结果如何。这里面我们想要得到的目标不是选择其中一个部分的一个action让当前reward最大化，我们的目标是要让reward的期望最大化，即在海量的trajectory中，平均起来策略$\pi$可以让reward的期望最大化。
#### 1. 2 Markov Chain定义

在讲Markov Decision Process之前先说一下Markov Chain。Markov Chain由两个部分组成，一个是state space，他可以是连续空间，也可以是离散空间。另一个是transistion operator，该operator有利于使用矩阵运算的形式表示状态的迁移，Markov Chain的格式如下图所示：
![image.png](https://upload-images.jianshu.io/upload_images/15463866-9ccb7a0cb295dbc1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
Markov Property的关键就如之前所说，当前state只由上一个state决定，而独立于以前的state。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-2f2e9670d0782670.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 1. 3 MDP定义

MDP,即Markov Decision Process。他与Markov Chain相比，多了Action，即多了Decision这个过程。这样transistion operator就不是之前那样只是矩阵运算，这时的operator就成了张量间的运算。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-0c45bd77f18384a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
其Markov Property同样改变，当前state由上一个state与action同时决定。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-a28ebf39292a2c96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

####  1.4 POMDP的定义

POMDP，即partially observed Markov decision process。这里强调的是部分观测，不是MDP的观察到整个状态。其中$\epsilon$表示了state到observation之前是概率分布形式。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-2b2553dc42cce676.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
---
# 2.RL的目标函数

强化学习的目标就是在整个的trajectory中，找到能够使得整个trajectory的期望reward最大的参数$\theta$。
![image.png](https://upload-images.jianshu.io/upload_images/15463866-6a4dfc27777161f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
其中如果把$\pi_θ(a_t∣s_t)p(s_{t+1})∣s_t,a_t)$看做一体，那么就可以当成是状态$s$到状态$s'$ 之间的状态转移概率，也就是让这个过程相当于是一个在(s,a)基础上的马尔科夫链。![image.png](https://upload-images.jianshu.io/upload_images/15463866-a044f8c01596dac6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

另外一种表达方式如下：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-aa30aa74a3129780.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 2.1 Finite horizon的情况

在有限时域的情况之下，我们对目标函数了一些改变。首先把一条轨迹的reward和在轨迹分布上的期望写成了在当前$(s_t,a_t)$ 下的边缘概率分布下期望的和。这么写是因为MDP可以看作是在(s,a)上的MC.

![image.png](https://upload-images.jianshu.io/upload_images/15463866-32d30b740c88ec5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



## 2.2 Infinite horizon的情况

对于无限时域的场景，下面看无限步的情况，对于之前考虑到的时域T，变成了infinite.

![image.png](https://upload-images.jianshu.io/upload_images/15463866-9a8e72e0bc12a69b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



这种情况下要考虑$p(s_t|a_t)$能否收敛到稳定的分布呢？无限时域里面可能不太好计算无限个当前状态动作反馈的期望和。但是在MC中，如果我们最终进行了无数次的转移，最终能够得到一个关于状态的稳定分布。而如之前所述MDP是可以看做的$(s_t,a_t)$马尔科夫链的。如$(s_t,a_t)$也存在这么一个稳定分布，就意味着状态转移之后得到的概率分布是相同的，即:
$$
\mu=T\mu
$$
这是说状态的分布是固定的，不是说转以后的状态不变。我们发现这样的$\mu$是存在的。

对上面的公式进行简单的线性代数计算，能够看到：
$$
(T-I)\mu=0
$$
其中$\mu$就是当T的特征值为1情况下的特征向量，而这个特征向量在某些情形下是一定存在的。这也就意味着在无限时域的情形下存在着一个关于$(s,a)$的稳定分布。有了这么一个分布存在，那么我们就能够计算无限时域下的reward期望的平均和。最终等价于是每一步对应的reward期望。

## 2.3 Finite与Infinite两种情况下的对比

![image.png](https://upload-images.jianshu.io/upload_images/15463866-df78fbefcfcb83bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 3. Alogrithm

我们从宏观上把强化学习看成了如下的循环：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-1763e436d45d3db3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

包含如下几个部分：

* 数据生成部分，比如运行某个策略不断生成新的数据，得到很多的trajectory。
* 模型拟合，根据生成的数据进行评估，评估结果当然有好也有坏。
* 改进算法，最后这一步就要根据之前的评估结果进行算法改进，改进的过程直观解释解释让产生好的评估结果的trajectory分布增加，减小那些让评估变差的trajectory的分布。
* 最后在根据改进的算法回到第一步，生成更多的数据，进行循环往复的改进。

## 3.1 开销分析

主要的三个步骤中，数据生成部分的开销在有些领域开销非常大，比如真实的机器人，自动驾驶这些实际应用的领域，获取数据很难。如果我们使用模拟环境，比如gym，mujoco这些去生成数据，这部分的开销又不会是瓶颈。

而对于第二部分的模型拟合评估，如果是简单的对奖励值进行叠加，那么就很简单。但是如果是需要使用model-based的方法来预测下一个状态从而进行评估，那这一步就开销就很大。

最后一步如果是简单的梯度下降就也很简单，如果是后向传播就相对复杂一些。所以究竟哪一步需要更大的开销依赖于算法本身。 

##  3.2 示例一

![image.png](https://upload-images.jianshu.io/upload_images/15463866-c559284512a75052.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们可以把fit model的部分做所有trajectory的reward的平均，在改进部分我马使用梯度下降算法进行更新。

## 3.3 示例2

![image-20191208161630072](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20191208161630072.png)

我们也可以用深度神经网络去做模型拟合，然后使用反向传播算法来更新模型参数。

 

# 4. 关于Expectation的处理



![image.png](https://upload-images.jianshu.io/upload_images/15463866-3da5f1ccbff51e31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在RL的目标函数中，我们总是遇到对reward求期望：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-f71f181bb52bda0a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在这里我们求期望得条件是$\tau \sim p_{\theta}$，他可以理解为对$(s,a )$进行两部分的分解，一个是$s_{t+1} \sim p(s_{t+1}|s_t,a_t)$,另一部分是$a_t \sim \pi (a_t |s_t)$.这样我们可以从第一个时间步开支展开期望，如下所示：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-b8d3c1ed661c3689.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中我们可以引导除**Q函数**，.

![image.png](https://upload-images.jianshu.io/upload_images/15463866-9336aaa5fbebd5bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这样我们的期望就被简化为：

![image-20191208165916204](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20191208165916204.png)

在这里如果$Q(s,a)$是已知的，那么我们就可以考虑通过改变$\pi(a,s)$来进行优化。比如:

![image.png](https://upload-images.jianshu.io/upload_images/15463866-214344deaf619db5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 4.1 Q function and Value Function

这里归总一下Q函数与值函数：

**Q函数**：其定义了从$s_t$开始执行$a_t$的情况下的期望总回报。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-3f7feae4ae300396.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**值函数**：其定义了从$s_t$开始的期望总回报。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-65c999e1a0a6b91a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**相互关系**:
$$
V^{\pi}(s_t)=E_{a_t \sim \pi(a_t|s_t)}[Q^{\pi}(s_t,a_t)]
$$

* 如果我们知道了策略$\pi$,并且为我们知道了Q函数$Q^{\pi}(s_t,a_t)$,那么我们就可以通过greedy的方式来改进$\pi$. 这种greedy的方式至少能够和$\pi$一样好。

* 我们可以通过计算梯度来增加我们认为好的action的概率，即如果$Q^{\pi}(s_t,a_t) > V^{\pi}(s)$，那么在这种情况下的action肯定是好于平均的表现（这里要注意到$V^{\pi}(s_t)=E_{a_t \sim \pi(a_t|s_t)}[Q^{\pi}(s_t,a_t)]$，即V体现的是期望值）。这样我们可以修改$\pi（a|s）$来增加好的action产生的概率。

# 5. 强化学习算法类型

根据我们之前定义的目标：

![image.png](https://upload-images.jianshu.io/upload_images/15463866-f3763bfe6e9b2830.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们把强化学习的算法分为几个类型：

* 基于梯度的算法（Policy_gradients），该类是直接对上述目标函数进行微分运算。
* 基于值得算法（Value-based），该类算法根据当前策略对值函数或者Q函数进行估计。
* Actor-critic，该类算法根据当前策略对值函数或者Q函数进行估计，然后用估计的结果来改进策略。
* 基于模型的算法（Model-based RL），该类算法是去估计模型的转移概率，然后用于规划，改进策略。

## 5.1 Model-based RL Alogrithms

该类算法主要考虑如何"improve the policy"。

![image.png](https://upload-images.jianshu.io/upload_images/15463866-2ca1843536a700fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* 他们可以使用模型进行规划，在连续空间的场景下，可以优化trajectory和做optimal control。在离散的情况下可以规划动作空间（比如MCTS: Monto Carlo tree search）
* 我们可以使用反向传播梯度进行策略的改进
* 我们可以使用模型去学习值函数，然后用学习到的值函数进行动态规划，或者为其他model-free方法生成更多的模拟数据。

## 5.2 Value-based RL Alogrithm

![image.png](https://upload-images.jianshu.io/upload_images/15463866-4c317cdd9900cf83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 5.3 Direct Policy Gradient

![image.png](https://upload-images.jianshu.io/upload_images/15463866-946d68e250893f5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 5.4 Actor-critic: value functions + policy gradients

![image.png](https://upload-images.jianshu.io/upload_images/15463866-58f453d5e4bf3506.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 5.5 不同算法之间的权衡

### 为什么会有如此多的 RL算法呢？

 主要是基于如下三个方面的考虑：

1. 利弊权衡：
   * 采样效率是否高？
   * 稳定性是否足够好？
   * 是否易于使用？
2. 问题假设：
   * 随机策略还是确定性策略？
   * 离散动作空间还是连续动作空间？
   * 有限时间步还是无限时间步？
3. 不同的问题的难易程度：
   * 策略是否易于表示？
   * 模型是否易于表示？

#### 关于采样效率

采样效率表达的是我们需要多少采样才能得到一个好的策略。这里我们最关心的是算法是否是off-policy的。

off-policy：就是使用非当前策略来生成采样来改进当前的策略。

on-policy: 每当策略有一点点改变 的时候，我们都需要为改进策略产生新的采样。

**不同算法的采样效率比较**

![image.png](https://upload-images.jianshu.io/upload_images/15463866-e9eb9f540943e891.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 关于稳定性与易用性

对于值函数拟合的算法，有些表现好的算法可以最小化Bellman error，但是也有些表现差的算法无法优化任何 东西，当前最流行的一些DRL的算法都无法保证收敛性。

对于基于模型的算法，一般他们都能够收敛，但是无法保证 收敛点就是我们需要的最好的模型。

对于基于梯度的算法， 这类算法是唯一直接将梯度下降或者梯度上升作用于目标函数的算法。

#### 关于假设

1. full observability假设：该假设通常是基于值函数类算法的前提。
2.  episodic learning假设：该假设通常是纯策略梯度方法算法的前提。
3. continuity or smoothness假设：该假设通常是一些连续值函数学习算法的前提。也是一些基于模型算法的前提假设。

## 5.6 简单的算法举例

这里列出来一些简单算法：

1. Value function fitting methods：

   *  Q-learning, DQN

   * Temporal difference learning
   * Fitted value iteration

2. Policy gradient methods

   * REINFORCE
   * Natural policy gradient
   * Trust Region Policy Optimization

3.  Actor-critic algorithms

   * Asynchronous advantage actor-critic (A3C)
   *  Soft actor-critic (SAC)

4. Model-based RL algorithms

   * Dyna
   * Guided policy search